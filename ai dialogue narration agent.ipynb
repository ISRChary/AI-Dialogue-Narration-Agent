{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c04c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3836167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CTCloss(tf.keras.losses.Loss):\n",
    "    \"\"\" CTCLoss objec for training the model\"\"\"\n",
    "    def __init__(self, name: str = \"CTCloss\") -> None:\n",
    "        super(CTCloss, self).__init__()\n",
    "        self.name = name\n",
    "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
    "\n",
    "    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor, sample_weight=None) -> tf.Tensor:\n",
    "        \"\"\" Compute the training batch CTC loss value\"\"\"\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928cc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CERMetric(tf.keras.metrics.Metric):\n",
    "    \"\"\"A custom TensorFlow metric to compute the Character Error Rate (CER).\n",
    "    \n",
    "    Args:\n",
    "        vocabulary: A string of the vocabulary used to encode the labels.\n",
    "        name: (Optional) string name of the metric instance.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabulary, name=\"CER\", **kwargs):\n",
    "        # Initialize the base Metric class\n",
    "        super(CERMetric, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        # Initialize variables to keep track of the cumulative character/word error rates and counter\n",
    "        self.cer_accumulator = tf.Variable(0.0, name=\"cer_accumulator\", dtype=tf.float32)\n",
    "        self.batch_counter = tf.Variable(0, name=\"batch_counter\", dtype=tf.int32)\n",
    "        \n",
    "        # Store the vocabulary as an attribute\n",
    "        self.vocabulary = tf.constant(list(vocabulary))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cer(pred_decoded, y_true, vocab, padding=-1):\n",
    "        \"\"\" Calculates the character error rate (CER) between the predicted labels and true labels for a batch of input data.\n",
    "\n",
    "        Args:\n",
    "            pred_decoded (tf.Tensor): The predicted labels, with dtype=tf.int32, usually output from tf.keras.backend.ctc_decode\n",
    "            y_true (tf.Tensor): The true labels, with dtype=tf.int32\n",
    "            vocab (tf.Tensor): The vocabulary tensor, with dtype=tf.string\n",
    "            padding (int, optional): The padding token when converting to sparse tensor. Defaults to -1.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The CER between the predicted labels and true labels\n",
    "        \"\"\"\n",
    "        # Keep only valid indices in the predicted labels tensor, replacing invalid indices with padding token\n",
    "        vocab_length = tf.cast(tf.shape(vocab)[0], tf.int64)\n",
    "        valid_pred_indices = tf.less(pred_decoded, vocab_length)\n",
    "        valid_pred = tf.where(valid_pred_indices, pred_decoded, padding)\n",
    "\n",
    "        # Keep only valid indices in the true labels tensor, replacing invalid indices with padding token\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        valid_true_indices = tf.less(y_true, vocab_length)\n",
    "        valid_true = tf.where(valid_true_indices, y_true, padding)\n",
    "\n",
    "        # Convert the valid predicted labels tensor to a sparse tensor\n",
    "        sparse_pred = tf.RaggedTensor.from_tensor(valid_pred, padding=padding).to_sparse()\n",
    "\n",
    "        # Convert the valid true labels tensor to a sparse tensor\n",
    "        sparse_true = tf.RaggedTensor.from_tensor(valid_true, padding=padding).to_sparse()\n",
    "\n",
    "        # Calculate the normalized edit distance between the sparse predicted labels tensor and sparse true labels tensor\n",
    "        distance = tf.edit_distance(sparse_pred, sparse_true, normalize=True)\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"Updates the state variables of the metric.\n",
    "\n",
    "        Args:\n",
    "            y_true: A tensor of true labels with shape (batch_size, sequence_length).\n",
    "            y_pred: A tensor of predicted labels with shape (batch_size, sequence_length, num_classes).\n",
    "            sample_weight: (Optional) a tensor of weights with shape (batch_size, sequence_length).\n",
    "        \"\"\"\n",
    "        # Get the input shape and length\n",
    "        input_shape = tf.keras.backend.shape(y_pred)\n",
    "        input_length = tf.ones(shape=input_shape[0], dtype=\"int32\") * tf.cast(input_shape[1], \"int32\")\n",
    "\n",
    "        # Decode the predicted labels using greedy decoding\n",
    "        decode_predicted, log = tf.keras.backend.ctc_decode(y_pred, input_length, greedy=True)\n",
    "\n",
    "        # Calculate the normalized edit distance between the predicted labels and true labels tensors\n",
    "        distance = self.get_cer(decode_predicted[0], y_true, self.vocabulary)\n",
    "\n",
    "        # Add the sum of the distance tensor to the cer_accumulator variable\n",
    "        self.cer_accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        \n",
    "        # Increment the batch_counter by the batch size\n",
    "        self.batch_counter.assign_add(input_shape[0])\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\" Computes and returns the metric result.\n",
    "\n",
    "        Returns:\n",
    "            A TensorFlow float representing the CER (character error rate).\n",
    "        \"\"\"\n",
    "        return tf.math.divide_no_nan(self.cer_accumulator, tf.cast(self.batch_counter, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cee996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WERMetric(tf.keras.metrics.Metric):\n",
    "    \"\"\"A custom TensorFlow metric to compute the Word Error Rate (WER).\n",
    "    \n",
    "    Attributes:\n",
    "        vocabulary: A string of the vocabulary used to encode the labels.\n",
    "        name: (Optional) string name of the metric instance.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabulary: str, name=\"WER\", **kwargs):\n",
    "        # Initialize the base Metric class\n",
    "        super(WERMetric, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        # Initialize variables to keep track of the cumulative character/word error rates and counter\n",
    "        self.wer_accumulator = tf.Variable(0.0, name=\"wer_accumulator\", dtype=tf.float32)\n",
    "        self.batch_counter = tf.Variable(0, name=\"batch_counter\", dtype=tf.int32)\n",
    "        \n",
    "        # Store the vocabulary as an attribute\n",
    "        self.vocabulary = tf.constant(list(vocabulary))\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_dense(dense_input: tf.Tensor, vocab: tf.Tensor, padding=-1, separator=\"\") -> tf.SparseTensor:\n",
    "        \"\"\" Preprocess the dense input tensor to a sparse tensor with given vocabulary\n",
    "        \n",
    "        Args:\n",
    "            dense_input (tf.Tensor): The dense input tensor, dtype=tf.int32\n",
    "            vocab (tf.Tensor): The vocabulary tensor, dtype=tf.string\n",
    "            padding (int, optional): The padding token when converting to sparse tensor. Defaults to -1.\n",
    "\n",
    "        Returns:\n",
    "            tf.SparseTensor: The sparse tensor with given vocabulary\n",
    "        \"\"\"\n",
    "        # Keep only the valid indices of the dense input tensor\n",
    "        vocab_length = tf.cast(tf.shape(vocab)[0], tf.int64)\n",
    "        dense_input = tf.cast(dense_input, tf.int64)\n",
    "        valid_indices = tf.less(dense_input, vocab_length)\n",
    "        valid_input = tf.where(valid_indices, dense_input, padding)\n",
    "\n",
    "        # Convert the valid input tensor to a ragged tensor with padding\n",
    "        input_ragged = tf.RaggedTensor.from_tensor(valid_input, padding=padding)\n",
    "\n",
    "        # Use the vocabulary tensor to get the strings corresponding to the indices in the ragged tensor\n",
    "        input_binary_chars = tf.gather(vocab, input_ragged)\n",
    "\n",
    "        # Join the binary character tensor along the sequence axis to get the input strings\n",
    "        input_strings = tf.strings.reduce_join(input_binary_chars, axis=1, separator=separator)\n",
    "\n",
    "        # Convert the input strings tensor to a sparse tensor\n",
    "        input_sparse_string = tf.strings.split(input_strings, sep=\" \").to_sparse()\n",
    "\n",
    "        return input_sparse_string\n",
    "\n",
    "    @staticmethod\n",
    "    def get_wer(pred_decoded, y_true, vocab, padding=-1, separator=\"\"):\n",
    "        \"\"\" Calculate the normalized WER distance between the predicted labels and true labels tensors\n",
    "\n",
    "        Args:\n",
    "            pred_decoded (tf.Tensor): The predicted labels tensor, dtype=tf.int32. Usually output from tf.keras.backend.ctc_decode\n",
    "            y_true (tf.Tensor): The true labels tensor, dtype=tf.int32\n",
    "            vocab (tf.Tensor): The vocabulary tensor, dtype=tf.string\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The normalized WER distance between the predicted labels and true labels tensors\n",
    "        \"\"\"\n",
    "        pred_sparse = WERMetric.preprocess_dense(pred_decoded, vocab, padding=padding, separator=separator)\n",
    "        true_sparse = WERMetric.preprocess_dense(y_true, vocab, padding=padding, separator=separator)\n",
    "\n",
    "        distance = tf.edit_distance(pred_sparse, true_sparse, normalize=True)\n",
    "\n",
    "        # test with numerical labels not string\n",
    "        # true_sparse = tf.RaggedTensor.from_tensor(y_true, padding=-1).to_sparse()\n",
    "\n",
    "        # replace 23 with -1\n",
    "        # pred_decoded2 = tf.where(tf.equal(pred_decoded, 23), -1, pred_decoded)\n",
    "        # pred_decoded2_sparse = tf.RaggedTensor.from_tensor(pred_decoded2, padding=-1).to_sparse()\n",
    "\n",
    "        # distance = tf.edit_distance(pred_decoded2_sparse, true_sparse, normalize=True)\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Get the input shape and length\n",
    "        input_shape = tf.keras.backend.shape(y_pred)\n",
    "        input_length = tf.ones(shape=input_shape[0], dtype=\"int32\") * tf.cast(input_shape[1], \"int32\")\n",
    "\n",
    "        # Decode the predicted labels using greedy decoding\n",
    "        decode_predicted, log = tf.keras.backend.ctc_decode(y_pred, input_length, greedy=True)\n",
    "\n",
    "        # Calculate the normalized edit distance between the predicted labels and true labels tensors\n",
    "        distance = self.get_wer(decode_predicted[0], y_true, self.vocabulary)\n",
    "\n",
    "        # Calculate the number of wrong words in batch and add to wer_accumulator variable\n",
    "        self.wer_accumulator.assign_add(tf.reduce_sum(tf.cast(distance, tf.float32)))\n",
    "\n",
    "        # Increment the batch_counter by the batch size\n",
    "        self.batch_counter.assign_add(input_shape[0])\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"Computes and returns the metric result.\n",
    "\n",
    "        Returns:\n",
    "            A TensorFlow float representing the WER (Word Error Rate).\n",
    "        \"\"\"\n",
    "        return tf.math.divide_no_nan(self.wer_accumulator, tf.cast(self.batch_counter, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "701223e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isrch\\.conda\\envs\\pyth42\\lib\\site-packages\\keras\\layers\\core\\lambda_layer.py:327: UserWarning: model is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  function = cls._parse_function_from_config(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import Loss\n",
    "# Define your custom loss function\n",
    "class CTCloss(Loss):\n",
    "    \"\"\" CTCLoss object for training the model\"\"\"\n",
    "    def __init__(self, name='CTCloss', reduction='auto') -> None:\n",
    "        super(CTCloss, self).__init__(name=name, reduction=reduction)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len,), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len,), dtype=\"int64\")\n",
    "\n",
    "        loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "        return loss\n",
    "\n",
    "# Load the model with the custom loss function\n",
    "model_path = \"C:/Users/isrch/AI Dialogue Narration Agent/model.h5\"\n",
    "vocabulary = \"abcdefghijklmnopqrstuvwxyz'?! \"\n",
    "model = load_model(model_path, custom_objects={'CTCloss': CTCloss(),'CERMetric': lambda **kwargs: CERMetric(vocabulary, **kwargs),'WERMetric': lambda **kwargs: WERMetric(vocabulary, **kwargs)})\n",
    "# Load the model with the custom loss function\n",
    "# model_path = \"\"C:\\Users\\isrch\\AI Dialogue Narration Agent\\model.h5\"\"\n",
    "# model = load_model(model_path, custom_objects={'custom_loss_function': CTCloss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1934e21f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1392, 193)]       0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 1392, 193, 1)      0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 696, 97, 32)       14432     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 696, 97, 32)      128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 696, 97, 32)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 696, 49, 32)       236544    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 696, 49, 32)      128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 696, 49, 32)       0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 696, 1568)         0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 696, 256)         1737728   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 696, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 696, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 696, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 696, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 696, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 696, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 696, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 696, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 696, 256)          65792     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 696, 256)          0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 696, 256)          0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 696, 31)           7967      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,639,679\n",
      "Trainable params: 3,639,551\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2643bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\isrch\\.conda\\envs\\pyth42\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c371d1dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/4-2-dataset\\\\dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m wavs_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwavs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Read metadata file and parse it\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m metadata_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m metadata_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Assign column names for the two columns\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Structure the dataset where each row is a list of [wav_file_path, sound transcription]\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pyth42\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pyth42\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\.conda\\envs\\pyth42\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pyth42\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\.conda\\envs\\pyth42\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/4-2-dataset\\\\dataset.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths (replace with actual paths)\n",
    "dataset_path = \"D:/4-2-dataset\"\n",
    "metadata_path = os.path.join(dataset_path,\"dataset.csv\")\n",
    "wavs_path = os.path.join(dataset_path, \"wavs\")\n",
    "\n",
    "# Read metadata file and parse it\n",
    "metadata_df = pd.read_csv(metadata_path, header=None)\n",
    "metadata_df.columns = [\"file_name\", \"transcription\"]  # Assign column names for the two columns\n",
    "\n",
    "# Structure the dataset where each row is a list of [wav_file_path, sound transcription]\n",
    "dataset = [[os.path.join(wavs_path, file), label.lower()] for file, label in tqdm(metadata_df.values)]\n",
    "\n",
    "# Create a ModelConfigs object to store model configurations\n",
    "configs = ModelConfigs()\n",
    "\n",
    "max_text_length, max_spectrogram_length = 0, 0\n",
    "for file_path, label in tqdm(dataset):\n",
    "    spectrogram = WavReader.get_spectrogram(file_path, frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length)\n",
    "    valid_label = [c for c in label if c in configs.vocab]\n",
    "    max_text_length = max(max_text_length, len(valid_label))\n",
    "    max_spectrogram_length = max(max_spectrogram_length, spectrogram.shape[0])\n",
    "    configs.input_shape = [max_spectrogram_length, spectrogram.shape[1]]\n",
    "\n",
    "configs.max_spectrogram_length = max_spectrogram_length\n",
    "configs.max_text_length = max_text_length\n",
    "configs.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcdd2a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtarfile\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlopen\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from mltu.preprocessors import WavReader\n",
    "\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.transformers import LabelIndexer, LabelPadding, SpectrogramPadding\n",
    "from mltu.tensorflow.losses import CTCloss\n",
    "from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n",
    "from mltu.tensorflow.metrics import CERMetric, WERMetric\n",
    "\n",
    "\n",
    "from configs import ModelConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc278cc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\4-2-dataset\\\\wavs\\\\Recording (1).wav', 'you always want to play games or get attention from me while iâ€™m studying or busy at work.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b59fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_provider = DataProvider(\n",
    "    dataset=dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[\n",
    "        WavReader(frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length),\n",
    "        ],\n",
    "    transformers=[\n",
    "        SpectrogramPadding(max_spectrogram_length=configs.max_spectrogram_length, padding_value=0),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n",
    "        ],\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data_provider, val_data_provider = data_provider.split(split = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7587ec5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1392, 193)]       0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 1392, 193, 1)      0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 696, 97, 32)       14432     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 696, 97, 32)      128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 696, 97, 32)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 696, 49, 32)       236544    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 696, 49, 32)      128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 696, 49, 32)       0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 696, 1568)         0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 696, 256)         1737728   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 696, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 696, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 696, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 696, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 696, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 696, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 696, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 696, 256)         394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 696, 256)          65792     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 696, 256)          0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 696, 256)          0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 696, 31)           7967      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,639,679\n",
      "Trainable params: 3,639,551\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate), \n",
    "    loss=CTCloss(), \n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "        ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f9ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(monitor=\"val_CER\", patience=20, verbose=1, mode=\"min\")\n",
    "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.h5\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "trainLogger = TrainLogger(configs.model_path)\n",
    "tb_callback = TensorBoard(f\"{configs.model_path}/logs\", update_freq=1)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_CER\", factor=0.8, min_delta=1e-10, patience=5, verbose=1, mode=\"auto\")\n",
    "model2onnx = Model2onnx(f\"{configs.model_path}/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e31ace3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 246.1420 - CER: 0.6385 - WER: 0.9752\n",
      "Epoch 1: val_CER improved from inf to 0.57776, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 180s 1s/step - loss: 246.1420 - CER: 0.6385 - WER: 0.9752 - val_loss: 171.8172 - val_CER: 0.5778 - val_WER: 0.9364 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 165.9286 - CER: 0.5681 - WER: 0.9439\n",
      "Epoch 2: val_CER improved from 0.57776 to 0.52088, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 153s 1s/step - loss: 165.9286 - CER: 0.5681 - WER: 0.9439 - val_loss: 147.3078 - val_CER: 0.5209 - val_WER: 0.8947 - lr: 5.0000e-04\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 146.1414 - CER: 0.5190 - WER: 0.9116\n",
      "Epoch 3: val_CER improved from 0.52088 to 0.48107, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 155s 1s/step - loss: 146.1414 - CER: 0.5190 - WER: 0.9116 - val_loss: 132.8582 - val_CER: 0.4811 - val_WER: 0.8667 - lr: 5.0000e-04\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 131.8121 - CER: 0.4760 - WER: 0.8881\n",
      "Epoch 4: val_CER improved from 0.48107 to 0.44644, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 157s 1s/step - loss: 131.8121 - CER: 0.4760 - WER: 0.8881 - val_loss: 122.3198 - val_CER: 0.4464 - val_WER: 0.8395 - lr: 5.0000e-04\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 120.1710 - CER: 0.4376 - WER: 0.8597\n",
      "Epoch 5: val_CER improved from 0.44644 to 0.41203, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 162s 1s/step - loss: 120.1710 - CER: 0.4376 - WER: 0.8597 - val_loss: 114.7828 - val_CER: 0.4120 - val_WER: 0.8116 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 111.1400 - CER: 0.4050 - WER: 0.8335\n",
      "Epoch 6: val_CER improved from 0.41203 to 0.38443, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 170s 1s/step - loss: 111.1400 - CER: 0.4050 - WER: 0.8335 - val_loss: 105.9876 - val_CER: 0.3844 - val_WER: 0.7857 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 102.2080 - CER: 0.3756 - WER: 0.8074\n",
      "Epoch 7: val_CER improved from 0.38443 to 0.35486, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 161s 1s/step - loss: 102.2080 - CER: 0.3756 - WER: 0.8074 - val_loss: 100.0406 - val_CER: 0.3549 - val_WER: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 94.5797 - CER: 0.3492 - WER: 0.7818\n",
      "Epoch 8: val_CER improved from 0.35486 to 0.32765, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 159s 1s/step - loss: 94.5797 - CER: 0.3492 - WER: 0.7818 - val_loss: 96.8748 - val_CER: 0.3276 - val_WER: 0.7266 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 87.7647 - CER: 0.3219 - WER: 0.7535\n",
      "Epoch 9: val_CER improved from 0.32765 to 0.31387, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 160s 1s/step - loss: 87.7647 - CER: 0.3219 - WER: 0.7535 - val_loss: 90.3620 - val_CER: 0.3139 - val_WER: 0.7090 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 82.3144 - CER: 0.3065 - WER: 0.7328\n",
      "Epoch 10: val_CER improved from 0.31387 to 0.29119, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 158s 1s/step - loss: 82.3144 - CER: 0.3065 - WER: 0.7328 - val_loss: 88.8310 - val_CER: 0.2912 - val_WER: 0.6885 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 76.6083 - CER: 0.2858 - WER: 0.7075\n",
      "Epoch 11: val_CER improved from 0.29119 to 0.27524, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 159s 1s/step - loss: 76.6083 - CER: 0.2858 - WER: 0.7075 - val_loss: 86.6192 - val_CER: 0.2752 - val_WER: 0.6680 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 71.4609 - CER: 0.2639 - WER: 0.6814\n",
      "Epoch 12: val_CER improved from 0.27524 to 0.26771, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 160s 1s/step - loss: 71.4609 - CER: 0.2639 - WER: 0.6814 - val_loss: 82.7175 - val_CER: 0.2677 - val_WER: 0.6574 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 66.4042 - CER: 0.2468 - WER: 0.6595\n",
      "Epoch 13: val_CER improved from 0.26771 to 0.25348, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 162s 1s/step - loss: 66.4042 - CER: 0.2468 - WER: 0.6595 - val_loss: 80.2500 - val_CER: 0.2535 - val_WER: 0.6392 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 62.6759 - CER: 0.2338 - WER: 0.6384\n",
      "Epoch 14: val_CER improved from 0.25348 to 0.24485, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 162s 1s/step - loss: 62.6759 - CER: 0.2338 - WER: 0.6384 - val_loss: 80.0087 - val_CER: 0.2448 - val_WER: 0.6074 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 58.1787 - CER: 0.2170 - WER: 0.6136\n",
      "Epoch 15: val_CER improved from 0.24485 to 0.24454, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 161s 1s/step - loss: 58.1787 - CER: 0.2170 - WER: 0.6136 - val_loss: 80.2599 - val_CER: 0.2445 - val_WER: 0.6105 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 55.5231 - CER: 0.2067 - WER: 0.5952\n",
      "Epoch 16: val_CER improved from 0.24454 to 0.23353, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 162s 1s/step - loss: 55.5231 - CER: 0.2067 - WER: 0.5952 - val_loss: 78.2558 - val_CER: 0.2335 - val_WER: 0.5911 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 52.0201 - CER: 0.1948 - WER: 0.5724\n",
      "Epoch 17: val_CER improved from 0.23353 to 0.22519, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 160s 1s/step - loss: 52.0201 - CER: 0.1948 - WER: 0.5724 - val_loss: 76.9262 - val_CER: 0.2252 - val_WER: 0.5720 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 49.3711 - CER: 0.1852 - WER: 0.5544\n",
      "Epoch 18: val_CER did not improve from 0.22519\n",
      "150/150 [==============================] - 162s 1s/step - loss: 49.3711 - CER: 0.1852 - WER: 0.5544 - val_loss: 78.7868 - val_CER: 0.2263 - val_WER: 0.5725 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 46.2025 - CER: 0.1719 - WER: 0.5312\n",
      "Epoch 19: val_CER improved from 0.22519 to 0.21946, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 161s 1s/step - loss: 46.2025 - CER: 0.1719 - WER: 0.5312 - val_loss: 76.1531 - val_CER: 0.2195 - val_WER: 0.5610 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 44.0686 - CER: 0.1669 - WER: 0.5257\n",
      "Epoch 20: val_CER improved from 0.21946 to 0.21418, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 160s 1s/step - loss: 44.0686 - CER: 0.1669 - WER: 0.5257 - val_loss: 76.8760 - val_CER: 0.2142 - val_WER: 0.5580 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 41.7322 - CER: 0.1557 - WER: 0.5022\n",
      "Epoch 21: val_CER improved from 0.21418 to 0.20693, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 160s 1s/step - loss: 41.7322 - CER: 0.1557 - WER: 0.5022 - val_loss: 72.8266 - val_CER: 0.2069 - val_WER: 0.5438 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 39.8325 - CER: 0.1505 - WER: 0.4909\n",
      "Epoch 22: val_CER improved from 0.20693 to 0.20432, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 160s 1s/step - loss: 39.8325 - CER: 0.1505 - WER: 0.4909 - val_loss: 73.8163 - val_CER: 0.2043 - val_WER: 0.5418 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 37.6353 - CER: 0.1421 - WER: 0.4752\n",
      "Epoch 23: val_CER improved from 0.20432 to 0.20001, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 186s 1s/step - loss: 37.6353 - CER: 0.1421 - WER: 0.4752 - val_loss: 73.7178 - val_CER: 0.2000 - val_WER: 0.5330 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 35.9142 - CER: 0.1373 - WER: 0.4616\n",
      "Epoch 24: val_CER improved from 0.20001 to 0.19859, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 164s 1s/step - loss: 35.9142 - CER: 0.1373 - WER: 0.4616 - val_loss: 74.8577 - val_CER: 0.1986 - val_WER: 0.5266 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 34.5492 - CER: 0.1322 - WER: 0.4598\n",
      "Epoch 25: val_CER did not improve from 0.19859\n",
      "150/150 [==============================] - 162s 1s/step - loss: 34.5492 - CER: 0.1322 - WER: 0.4598 - val_loss: 74.1991 - val_CER: 0.1990 - val_WER: 0.5289 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 32.6768 - CER: 0.1253 - WER: 0.4349\n",
      "Epoch 26: val_CER improved from 0.19859 to 0.19802, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 164s 1s/step - loss: 32.6768 - CER: 0.1253 - WER: 0.4349 - val_loss: 76.4467 - val_CER: 0.1980 - val_WER: 0.5220 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 31.4473 - CER: 0.1215 - WER: 0.4251\n",
      "Epoch 27: val_CER improved from 0.19802 to 0.19448, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 163s 1s/step - loss: 31.4473 - CER: 0.1215 - WER: 0.4251 - val_loss: 73.9642 - val_CER: 0.1945 - val_WER: 0.5193 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 29.9875 - CER: 0.1142 - WER: 0.4080\n",
      "Epoch 28: val_CER improved from 0.19448 to 0.18847, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 164s 1s/step - loss: 29.9875 - CER: 0.1142 - WER: 0.4080 - val_loss: 73.7365 - val_CER: 0.1885 - val_WER: 0.5047 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 29.5474 - CER: 0.1126 - WER: 0.4086\n",
      "Epoch 29: val_CER improved from 0.18847 to 0.18761, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 178s 1s/step - loss: 29.5474 - CER: 0.1126 - WER: 0.4086 - val_loss: 72.7287 - val_CER: 0.1876 - val_WER: 0.4927 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 27.9511 - CER: 0.1070 - WER: 0.3930\n",
      "Epoch 30: val_CER improved from 0.18761 to 0.18491, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 176s 1s/step - loss: 27.9511 - CER: 0.1070 - WER: 0.3930 - val_loss: 76.2808 - val_CER: 0.1849 - val_WER: 0.4876 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 26.3398 - CER: 0.1004 - WER: 0.3765\n",
      "Epoch 31: val_CER improved from 0.18491 to 0.18141, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 161s 1s/step - loss: 26.3398 - CER: 0.1004 - WER: 0.3765 - val_loss: 74.5511 - val_CER: 0.1814 - val_WER: 0.4838 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 25.2062 - CER: 0.0970 - WER: 0.3690\n",
      "Epoch 32: val_CER improved from 0.18141 to 0.18101, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 171s 1s/step - loss: 25.2062 - CER: 0.0970 - WER: 0.3690 - val_loss: 75.3952 - val_CER: 0.1810 - val_WER: 0.4899 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 24.0739 - CER: 0.0933 - WER: 0.3533\n",
      "Epoch 33: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 169s 1s/step - loss: 24.0739 - CER: 0.0933 - WER: 0.3533 - val_loss: 74.9572 - val_CER: 0.1814 - val_WER: 0.4848 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 58.0113 - CER: 0.2081 - WER: 0.5691\n",
      "Epoch 34: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 172s 1s/step - loss: 58.0113 - CER: 0.2081 - WER: 0.5691 - val_loss: 77.6103 - val_CER: 0.2408 - val_WER: 0.5973 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 52.7227 - CER: 0.1962 - WER: 0.5632\n",
      "Epoch 35: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 190s 1s/step - loss: 52.7227 - CER: 0.1962 - WER: 0.5632 - val_loss: 72.8946 - val_CER: 0.2203 - val_WER: 0.5546 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 45.6533 - CER: 0.1718 - WER: 0.5195\n",
      "Epoch 36: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 164s 1s/step - loss: 45.6533 - CER: 0.1718 - WER: 0.5195 - val_loss: 72.6771 - val_CER: 0.2055 - val_WER: 0.5410 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 41.0449 - CER: 0.1556 - WER: 0.4915\n",
      "Epoch 37: val_CER did not improve from 0.18101\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "150/150 [==============================] - 163s 1s/step - loss: 41.0449 - CER: 0.1556 - WER: 0.4915 - val_loss: 72.2249 - val_CER: 0.2045 - val_WER: 0.5301 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 36.3790 - CER: 0.1382 - WER: 0.4589\n",
      "Epoch 38: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 171s 1s/step - loss: 36.3790 - CER: 0.1382 - WER: 0.4589 - val_loss: 72.0162 - val_CER: 0.1960 - val_WER: 0.5236 - lr: 4.0000e-04\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 33.7987 - CER: 0.1300 - WER: 0.4322\n",
      "Epoch 39: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 163s 1s/step - loss: 33.7987 - CER: 0.1300 - WER: 0.4322 - val_loss: 72.5044 - val_CER: 0.1909 - val_WER: 0.5036 - lr: 4.0000e-04\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 32.4461 - CER: 0.1259 - WER: 0.4321\n",
      "Epoch 40: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 165s 1s/step - loss: 32.4461 - CER: 0.1259 - WER: 0.4321 - val_loss: 70.1275 - val_CER: 0.1860 - val_WER: 0.5035 - lr: 4.0000e-04\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 31.0037 - CER: 0.1181 - WER: 0.4134\n",
      "Epoch 41: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 177s 1s/step - loss: 31.0037 - CER: 0.1181 - WER: 0.4134 - val_loss: 68.0889 - val_CER: 0.1870 - val_WER: 0.5102 - lr: 4.0000e-04\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 29.0688 - CER: 0.1129 - WER: 0.3991\n",
      "Epoch 42: val_CER did not improve from 0.18101\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.00032000001519918444.\n",
      "150/150 [==============================] - 168s 1s/step - loss: 29.0688 - CER: 0.1129 - WER: 0.3991 - val_loss: 69.9851 - val_CER: 0.1856 - val_WER: 0.5007 - lr: 4.0000e-04\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 27.1590 - CER: 0.1068 - WER: 0.3899\n",
      "Epoch 43: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 162s 1s/step - loss: 27.1590 - CER: 0.1068 - WER: 0.3899 - val_loss: 72.4954 - val_CER: 0.1851 - val_WER: 0.5014 - lr: 3.2000e-04\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 26.5203 - CER: 0.1026 - WER: 0.3776\n",
      "Epoch 44: val_CER did not improve from 0.18101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 163s 1s/step - loss: 26.5203 - CER: 0.1026 - WER: 0.3776 - val_loss: 71.6406 - val_CER: 0.1817 - val_WER: 0.4855 - lr: 3.2000e-04\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 25.2687 - CER: 0.1010 - WER: 0.3733\n",
      "Epoch 45: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 163s 1s/step - loss: 25.2687 - CER: 0.1010 - WER: 0.3733 - val_loss: 73.1365 - val_CER: 0.1818 - val_WER: 0.4921 - lr: 3.2000e-04\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 24.3070 - CER: 0.0964 - WER: 0.3592\n",
      "Epoch 46: val_CER did not improve from 0.18101\n",
      "150/150 [==============================] - 167s 1s/step - loss: 24.3070 - CER: 0.0964 - WER: 0.3592 - val_loss: 71.3724 - val_CER: 0.1810 - val_WER: 0.4801 - lr: 3.2000e-04\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 23.5311 - CER: 0.0926 - WER: 0.3514\n",
      "Epoch 47: val_CER improved from 0.18101 to 0.17837, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 164s 1s/step - loss: 23.5311 - CER: 0.0926 - WER: 0.3514 - val_loss: 71.9748 - val_CER: 0.1784 - val_WER: 0.4730 - lr: 3.2000e-04\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 22.8782 - CER: 0.0919 - WER: 0.3513\n",
      "Epoch 48: val_CER did not improve from 0.17837\n",
      "150/150 [==============================] - 163s 1s/step - loss: 22.8782 - CER: 0.0919 - WER: 0.3513 - val_loss: 75.2635 - val_CER: 0.1804 - val_WER: 0.4798 - lr: 3.2000e-04\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 22.4211 - CER: 0.0884 - WER: 0.3392\n",
      "Epoch 49: val_CER improved from 0.17837 to 0.17703, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 166s 1s/step - loss: 22.4211 - CER: 0.0884 - WER: 0.3392 - val_loss: 73.0877 - val_CER: 0.1770 - val_WER: 0.4660 - lr: 3.2000e-04\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 21.8940 - CER: 0.0870 - WER: 0.3351\n",
      "Epoch 50: val_CER improved from 0.17703 to 0.17587, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 164s 1s/step - loss: 21.8940 - CER: 0.0870 - WER: 0.3351 - val_loss: 73.9970 - val_CER: 0.1759 - val_WER: 0.4588 - lr: 3.2000e-04\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 21.4099 - CER: 0.0838 - WER: 0.3277\n",
      "Epoch 51: val_CER improved from 0.17587 to 0.17420, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 165s 1s/step - loss: 21.4099 - CER: 0.0838 - WER: 0.3277 - val_loss: 71.7611 - val_CER: 0.1742 - val_WER: 0.4774 - lr: 3.2000e-04\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 20.8882 - CER: 0.0833 - WER: 0.3283\n",
      "Epoch 52: val_CER did not improve from 0.17420\n",
      "150/150 [==============================] - 163s 1s/step - loss: 20.8882 - CER: 0.0833 - WER: 0.3283 - val_loss: 75.1610 - val_CER: 0.1804 - val_WER: 0.4826 - lr: 3.2000e-04\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 20.2416 - CER: 0.0795 - WER: 0.3142\n",
      "Epoch 53: val_CER did not improve from 0.17420\n",
      "150/150 [==============================] - 163s 1s/step - loss: 20.2416 - CER: 0.0795 - WER: 0.3142 - val_loss: 75.5791 - val_CER: 0.1796 - val_WER: 0.4771 - lr: 3.2000e-04\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 19.7113 - CER: 0.0772 - WER: 0.3093\n",
      "Epoch 54: val_CER improved from 0.17420 to 0.17163, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 163s 1s/step - loss: 19.7113 - CER: 0.0772 - WER: 0.3093 - val_loss: 74.3518 - val_CER: 0.1716 - val_WER: 0.4600 - lr: 3.2000e-04\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 18.8183 - CER: 0.0748 - WER: 0.3034\n",
      "Epoch 55: val_CER improved from 0.17163 to 0.17087, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 163s 1s/step - loss: 18.8183 - CER: 0.0748 - WER: 0.3034 - val_loss: 75.9512 - val_CER: 0.1709 - val_WER: 0.4705 - lr: 3.2000e-04\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 18.7017 - CER: 0.0753 - WER: 0.3039\n",
      "Epoch 56: val_CER improved from 0.17087 to 0.16939, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 163s 1s/step - loss: 18.7017 - CER: 0.0753 - WER: 0.3039 - val_loss: 75.2531 - val_CER: 0.1694 - val_WER: 0.4606 - lr: 3.2000e-04\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 17.9580 - CER: 0.0724 - WER: 0.2917\n",
      "Epoch 57: val_CER did not improve from 0.16939\n",
      "150/150 [==============================] - 166s 1s/step - loss: 17.9580 - CER: 0.0724 - WER: 0.2917 - val_loss: 77.0501 - val_CER: 0.1730 - val_WER: 0.4688 - lr: 3.2000e-04\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 17.7751 - CER: 0.0707 - WER: 0.2870\n",
      "Epoch 58: val_CER did not improve from 0.16939\n",
      "150/150 [==============================] - 164s 1s/step - loss: 17.7751 - CER: 0.0707 - WER: 0.2870 - val_loss: 75.4913 - val_CER: 0.1718 - val_WER: 0.4681 - lr: 3.2000e-04\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 17.3207 - CER: 0.0688 - WER: 0.2813\n",
      "Epoch 59: val_CER did not improve from 0.16939\n",
      "150/150 [==============================] - 174s 1s/step - loss: 17.3207 - CER: 0.0688 - WER: 0.2813 - val_loss: 76.1979 - val_CER: 0.1731 - val_WER: 0.4698 - lr: 3.2000e-04\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 17.2061 - CER: 0.0695 - WER: 0.2844\n",
      "Epoch 60: val_CER improved from 0.16939 to 0.16817, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 171s 1s/step - loss: 17.2061 - CER: 0.0695 - WER: 0.2844 - val_loss: 76.0255 - val_CER: 0.1682 - val_WER: 0.4607 - lr: 3.2000e-04\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 16.9275 - CER: 0.0679 - WER: 0.2805\n",
      "Epoch 61: val_CER did not improve from 0.16817\n",
      "150/150 [==============================] - 164s 1s/step - loss: 16.9275 - CER: 0.0679 - WER: 0.2805 - val_loss: 73.3374 - val_CER: 0.1695 - val_WER: 0.4587 - lr: 3.2000e-04\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 16.2105 - CER: 0.0657 - WER: 0.2742\n",
      "Epoch 62: val_CER did not improve from 0.16817\n",
      "150/150 [==============================] - 165s 1s/step - loss: 16.2105 - CER: 0.0657 - WER: 0.2742 - val_loss: 78.7400 - val_CER: 0.1749 - val_WER: 0.4707 - lr: 3.2000e-04\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 15.8428 - CER: 0.0642 - WER: 0.2706\n",
      "Epoch 63: val_CER did not improve from 0.16817\n",
      "150/150 [==============================] - 164s 1s/step - loss: 15.8428 - CER: 0.0642 - WER: 0.2706 - val_loss: 77.6690 - val_CER: 0.1701 - val_WER: 0.4664 - lr: 3.2000e-04\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 15.7222 - CER: 0.0640 - WER: 0.2690\n",
      "Epoch 64: val_CER did not improve from 0.16817\n",
      "150/150 [==============================] - 164s 1s/step - loss: 15.7222 - CER: 0.0640 - WER: 0.2690 - val_loss: 76.5808 - val_CER: 0.1683 - val_WER: 0.4636 - lr: 3.2000e-04\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 15.4508 - CER: 0.0630 - WER: 0.2681\n",
      "Epoch 65: val_CER did not improve from 0.16817\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0002560000168159604.\n",
      "150/150 [==============================] - 164s 1s/step - loss: 15.4508 - CER: 0.0630 - WER: 0.2681 - val_loss: 77.2927 - val_CER: 0.1709 - val_WER: 0.4645 - lr: 3.2000e-04\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 14.3655 - CER: 0.0593 - WER: 0.2485\n",
      "Epoch 66: val_CER improved from 0.16817 to 0.16313, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 164s 1s/step - loss: 14.3655 - CER: 0.0593 - WER: 0.2485 - val_loss: 75.6278 - val_CER: 0.1631 - val_WER: 0.4503 - lr: 2.5600e-04\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 13.9834 - CER: 0.0576 - WER: 0.2494\n",
      "Epoch 67: val_CER did not improve from 0.16313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 165s 1s/step - loss: 13.9834 - CER: 0.0576 - WER: 0.2494 - val_loss: 76.8034 - val_CER: 0.1650 - val_WER: 0.4534 - lr: 2.5600e-04\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 13.6754 - CER: 0.0562 - WER: 0.2445\n",
      "Epoch 68: val_CER did not improve from 0.16313\n",
      "150/150 [==============================] - 167s 1s/step - loss: 13.6754 - CER: 0.0562 - WER: 0.2445 - val_loss: 77.5129 - val_CER: 0.1670 - val_WER: 0.4648 - lr: 2.5600e-04\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 13.0031 - CER: 0.0545 - WER: 0.2385\n",
      "Epoch 69: val_CER did not improve from 0.16313\n",
      "150/150 [==============================] - 164s 1s/step - loss: 13.0031 - CER: 0.0545 - WER: 0.2385 - val_loss: 80.4368 - val_CER: 0.1673 - val_WER: 0.4584 - lr: 2.5600e-04\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 13.3245 - CER: 0.0550 - WER: 0.2394\n",
      "Epoch 70: val_CER did not improve from 0.16313\n",
      "150/150 [==============================] - 164s 1s/step - loss: 13.3245 - CER: 0.0550 - WER: 0.2394 - val_loss: 81.0660 - val_CER: 0.1676 - val_WER: 0.4638 - lr: 2.5600e-04\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 12.7845 - CER: 0.0537 - WER: 0.2363\n",
      "Epoch 71: val_CER improved from 0.16313 to 0.16017, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 164s 1s/step - loss: 12.7845 - CER: 0.0537 - WER: 0.2363 - val_loss: 79.6051 - val_CER: 0.1602 - val_WER: 0.4492 - lr: 2.5600e-04\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 12.7893 - CER: 0.0526 - WER: 0.2315\n",
      "Epoch 72: val_CER did not improve from 0.16017\n",
      "150/150 [==============================] - 163s 1s/step - loss: 12.7893 - CER: 0.0526 - WER: 0.2315 - val_loss: 79.6154 - val_CER: 0.1651 - val_WER: 0.4470 - lr: 2.5600e-04\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 12.5893 - CER: 0.0517 - WER: 0.2304\n",
      "Epoch 73: val_CER did not improve from 0.16017\n",
      "150/150 [==============================] - 165s 1s/step - loss: 12.5893 - CER: 0.0517 - WER: 0.2304 - val_loss: 81.2131 - val_CER: 0.1653 - val_WER: 0.4465 - lr: 2.5600e-04\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 12.2066 - CER: 0.0510 - WER: 0.2272\n",
      "Epoch 74: val_CER did not improve from 0.16017\n",
      "150/150 [==============================] - 167s 1s/step - loss: 12.2066 - CER: 0.0510 - WER: 0.2272 - val_loss: 78.5886 - val_CER: 0.1621 - val_WER: 0.4516 - lr: 2.5600e-04\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 12.0748 - CER: 0.0501 - WER: 0.2214\n",
      "Epoch 75: val_CER did not improve from 0.16017\n",
      "150/150 [==============================] - 164s 1s/step - loss: 12.0748 - CER: 0.0501 - WER: 0.2214 - val_loss: 79.3226 - val_CER: 0.1626 - val_WER: 0.4455 - lr: 2.5600e-04\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 11.6250 - CER: 0.0483 - WER: 0.2169\n",
      "Epoch 76: val_CER did not improve from 0.16017\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.00020480002276599408.\n",
      "150/150 [==============================] - 165s 1s/step - loss: 11.6250 - CER: 0.0483 - WER: 0.2169 - val_loss: 79.2371 - val_CER: 0.1616 - val_WER: 0.4441 - lr: 2.5600e-04\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 11.5077 - CER: 0.0495 - WER: 0.2223\n",
      "Epoch 77: val_CER did not improve from 0.16017\n",
      "150/150 [==============================] - 165s 1s/step - loss: 11.5077 - CER: 0.0495 - WER: 0.2223 - val_loss: 79.4270 - val_CER: 0.1625 - val_WER: 0.4438 - lr: 2.0480e-04\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 11.2527 - CER: 0.0477 - WER: 0.2115\n",
      "Epoch 78: val_CER did not improve from 0.16017\n",
      "150/150 [==============================] - 165s 1s/step - loss: 11.2527 - CER: 0.0477 - WER: 0.2115 - val_loss: 81.1536 - val_CER: 0.1607 - val_WER: 0.4434 - lr: 2.0480e-04\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 10.7661 - CER: 0.0457 - WER: 0.2046\n",
      "Epoch 79: val_CER did not improve from 0.16017\n",
      "150/150 [==============================] - 168s 1s/step - loss: 10.7661 - CER: 0.0457 - WER: 0.2046 - val_loss: 82.7163 - val_CER: 0.1608 - val_WER: 0.4436 - lr: 2.0480e-04\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 10.8325 - CER: 0.0463 - WER: 0.2093\n",
      "Epoch 80: val_CER improved from 0.16017 to 0.15849, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 167s 1s/step - loss: 10.8325 - CER: 0.0463 - WER: 0.2093 - val_loss: 81.6978 - val_CER: 0.1585 - val_WER: 0.4348 - lr: 2.0480e-04\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 10.8833 - CER: 0.0456 - WER: 0.2064\n",
      "Epoch 81: val_CER improved from 0.15849 to 0.15704, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 165s 1s/step - loss: 10.8833 - CER: 0.0456 - WER: 0.2064 - val_loss: 80.9731 - val_CER: 0.1570 - val_WER: 0.4435 - lr: 2.0480e-04\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 10.6822 - CER: 0.0443 - WER: 0.2017\n",
      "Epoch 82: val_CER did not improve from 0.15704\n",
      "150/150 [==============================] - 166s 1s/step - loss: 10.6822 - CER: 0.0443 - WER: 0.2017 - val_loss: 81.0003 - val_CER: 0.1588 - val_WER: 0.4428 - lr: 2.0480e-04\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 10.5036 - CER: 0.0451 - WER: 0.2072\n",
      "Epoch 83: val_CER improved from 0.15704 to 0.15685, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 165s 1s/step - loss: 10.5036 - CER: 0.0451 - WER: 0.2072 - val_loss: 79.4050 - val_CER: 0.1568 - val_WER: 0.4398 - lr: 2.0480e-04\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 10.3092 - CER: 0.0427 - WER: 0.1937\n",
      "Epoch 84: val_CER did not improve from 0.15685\n",
      "150/150 [==============================] - 165s 1s/step - loss: 10.3092 - CER: 0.0427 - WER: 0.1937 - val_loss: 81.9848 - val_CER: 0.1607 - val_WER: 0.4397 - lr: 2.0480e-04\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 10.0624 - CER: 0.0431 - WER: 0.1949\n",
      "Epoch 85: val_CER did not improve from 0.15685\n",
      "150/150 [==============================] - 170s 1s/step - loss: 10.0624 - CER: 0.0431 - WER: 0.1949 - val_loss: 81.4842 - val_CER: 0.1580 - val_WER: 0.4326 - lr: 2.0480e-04\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 9.8557 - CER: 0.0419 - WER: 0.1921\n",
      "Epoch 86: val_CER did not improve from 0.15685\n",
      "150/150 [==============================] - 168s 1s/step - loss: 9.8557 - CER: 0.0419 - WER: 0.1921 - val_loss: 83.4353 - val_CER: 0.1580 - val_WER: 0.4333 - lr: 2.0480e-04\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 9.6707 - CER: 0.0407 - WER: 0.1874\n",
      "Epoch 87: val_CER did not improve from 0.15685\n",
      "150/150 [==============================] - 166s 1s/step - loss: 9.6707 - CER: 0.0407 - WER: 0.1874 - val_loss: 83.2367 - val_CER: 0.1609 - val_WER: 0.4410 - lr: 2.0480e-04\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 9.4735 - CER: 0.0407 - WER: 0.1864\n",
      "Epoch 88: val_CER improved from 0.15685 to 0.15527, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 171s 1s/step - loss: 9.4735 - CER: 0.0407 - WER: 0.1864 - val_loss: 82.1629 - val_CER: 0.1553 - val_WER: 0.4371 - lr: 2.0480e-04\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 9.1949 - CER: 0.0399 - WER: 0.1869\n",
      "Epoch 89: val_CER did not improve from 0.15527\n",
      "150/150 [==============================] - 176s 1s/step - loss: 9.1949 - CER: 0.0399 - WER: 0.1869 - val_loss: 84.4406 - val_CER: 0.1585 - val_WER: 0.4391 - lr: 2.0480e-04\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 9.3657 - CER: 0.0404 - WER: 0.1838\n",
      "Epoch 90: val_CER did not improve from 0.15527\n",
      "150/150 [==============================] - 178s 1s/step - loss: 9.3657 - CER: 0.0404 - WER: 0.1838 - val_loss: 83.7444 - val_CER: 0.1571 - val_WER: 0.4436 - lr: 2.0480e-04\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 9.1914 - CER: 0.0394 - WER: 0.1845\n",
      "Epoch 91: val_CER did not improve from 0.15527\n",
      "150/150 [==============================] - 172s 1s/step - loss: 9.1914 - CER: 0.0394 - WER: 0.1845 - val_loss: 83.5981 - val_CER: 0.1561 - val_WER: 0.4341 - lr: 2.0480e-04\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 9.1545 - CER: 0.0394 - WER: 0.1846\n",
      "Epoch 92: val_CER did not improve from 0.15527\n",
      "150/150 [==============================] - 167s 1s/step - loss: 9.1545 - CER: 0.0394 - WER: 0.1846 - val_loss: 83.8014 - val_CER: 0.1561 - val_WER: 0.4359 - lr: 2.0480e-04\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 8.7821 - CER: 0.0382 - WER: 0.1782\n",
      "Epoch 93: val_CER did not improve from 0.15527\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.00016384001355618238.\n",
      "150/150 [==============================] - 165s 1s/step - loss: 8.7821 - CER: 0.0382 - WER: 0.1782 - val_loss: 85.8268 - val_CER: 0.1585 - val_WER: 0.4465 - lr: 2.0480e-04\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 8.5175 - CER: 0.0372 - WER: 0.1759\n",
      "Epoch 94: val_CER did not improve from 0.15527\n",
      "150/150 [==============================] - 165s 1s/step - loss: 8.5175 - CER: 0.0372 - WER: 0.1759 - val_loss: 83.9059 - val_CER: 0.1567 - val_WER: 0.4398 - lr: 1.6384e-04\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 8.4271 - CER: 0.0364 - WER: 0.1711\n",
      "Epoch 95: val_CER did not improve from 0.15527\n",
      "150/150 [==============================] - 166s 1s/step - loss: 8.4271 - CER: 0.0364 - WER: 0.1711 - val_loss: 85.4870 - val_CER: 0.1578 - val_WER: 0.4377 - lr: 1.6384e-04\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 8.2992 - CER: 0.0360 - WER: 0.1685\n",
      "Epoch 96: val_CER did not improve from 0.15527\n",
      "150/150 [==============================] - 165s 1s/step - loss: 8.2992 - CER: 0.0360 - WER: 0.1685 - val_loss: 84.9714 - val_CER: 0.1555 - val_WER: 0.4386 - lr: 1.6384e-04\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 8.2328 - CER: 0.0351 - WER: 0.1678\n",
      "Epoch 97: val_CER improved from 0.15527 to 0.15319, saving model to Models/05_sound_to_text\\202404231811\\model.h5\n",
      "150/150 [==============================] - 163s 1s/step - loss: 8.2328 - CER: 0.0351 - WER: 0.1678 - val_loss: 85.5985 - val_CER: 0.1532 - val_WER: 0.4296 - lr: 1.6384e-04\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 7.8309 - CER: 0.0347 - WER: 0.1624\n",
      "Epoch 98: val_CER did not improve from 0.15319\n",
      "150/150 [==============================] - 167s 1s/step - loss: 7.8309 - CER: 0.0347 - WER: 0.1624 - val_loss: 87.1886 - val_CER: 0.1549 - val_WER: 0.4312 - lr: 1.6384e-04\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 8.0714 - CER: 0.0352 - WER: 0.1635\n",
      "Epoch 99: val_CER did not improve from 0.15319\n",
      "150/150 [==============================] - 166s 1s/step - loss: 8.0714 - CER: 0.0352 - WER: 0.1635 - val_loss: 83.7057 - val_CER: 0.1535 - val_WER: 0.4315 - lr: 1.6384e-04\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 7.9227 - CER: 0.0349 - WER: 0.1636\n",
      "Epoch 100: val_CER did not improve from 0.15319\n",
      "150/150 [==============================] - 166s 1s/step - loss: 7.9227 - CER: 0.0349 - WER: 0.1636 - val_loss: 87.0918 - val_CER: 0.1539 - val_WER: 0.4318 - lr: 1.6384e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21818607250>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=100,\n",
    "    callbacks=[earlystopper,checkpoint, trainLogger, reduceLROnPlat, tb_callback, model2onnx],\n",
    "    workers=configs.train_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce0b97b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1310 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WavToTextModel' object has no attribute 'input_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 54\u001b[0m\n\u001b[0;32m     48\u001b[0m             spectrogram \u001b[38;5;241m=\u001b[39m spectrogram[:\u001b[38;5;241m1392\u001b[39m, :]\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;66;03m# Plot spectrogram\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#         WavReader.plot_spectrogram(spectrogram, label)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectrogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;66;03m# Calculate CER and WER\u001b[39;00m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(text)\n",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m, in \u001b[0;36mWavToTextModel.predict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     15\u001b[0m     data_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mNone\u001b[39;00m, {\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_name\u001b[49m: data_pred})[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m     text \u001b[38;5;241m=\u001b[39m ctc_decoder(preds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_list)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WavToTextModel' object has no attribute 'input_name'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from mltu.configs import BaseModelConfigs\n",
    "from mltu.inferenceModel import OnnxInferenceModel\n",
    "from mltu.preprocessors import WavReader\n",
    "from mltu.utils.text_utils import ctc_decoder, get_cer, get_wer\n",
    "\n",
    "class WavToTextModel(OnnxInferenceModel):\n",
    "    def _init_(self, char_list: str, *args, **kwargs):\n",
    "        super()._init_(*args, **kwargs)\n",
    "        self.char_list = char_list\n",
    "\n",
    "    def predict(self, data: np.ndarray):\n",
    "        data_pred = np.expand_dims(data, axis=0)\n",
    "        preds = self.model.run(None, {self.input_name: data_pred})[0]\n",
    "        text = ctc_decoder(preds, self.char_list)[0]\n",
    "        return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model configurations\n",
    "    configs = BaseModelConfigs.load(r\"C:\\Users\\isrch\\AI Dialogue Narration Agent\\Models\\05_sound_to_text\\202404231811\\configs.yaml\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = WavToTextModel(model_path=configs.model_path, char_list=configs.vocab, force_cpu=False)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(r\"C:\\Users\\isrch\\AI Dialogue Narration Agent\\val.csv\").values.tolist()\n",
    "\n",
    "    # Initialize lists to store CER and WER\n",
    "    accum_cer, accum_wer = [], []\n",
    "\n",
    "    # Iterate over data\n",
    "    for wav_path, label in tqdm(df):\n",
    "        wav_path = wav_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "        # Get spectrogram\n",
    "        spectrogram = WavReader.get_spectrogram(wav_path, frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length)\n",
    "\n",
    "        # Plot raw audio\n",
    "#         WavReader.plot_raw_audio(wav_path, label)\n",
    "\n",
    "        # Adjust spectrogram size\n",
    "        if spectrogram.shape[0] < 1392:\n",
    "            padding = 1392 - spectrogram.shape[0]\n",
    "            spectrogram = np.pad(spectrogram, ((0, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "        elif spectrogram.shape[0] > 1392:\n",
    "            spectrogram = spectrogram[:1392, :]\n",
    "\n",
    "        # Plot spectrogram\n",
    "#         WavReader.plot_spectrogram(spectrogram, label)\n",
    "\n",
    "        # Make prediction\n",
    "        text = model.predict(spectrogram)\n",
    "\n",
    "        # Calculate CER and WER\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ae5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
